{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acute-persian",
   "metadata": {},
   "source": [
    "# *Covid - 19 Twitter Analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "optimum-banks",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:20:35.746860Z",
     "start_time": "2021-02-10T06:20:34.114376Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-council",
   "metadata": {},
   "source": [
    "## *Text Processing*\n",
    "### Text Processing Steps:\n",
    "- Converting Upper or title case letters to lower case\n",
    "- Removing Escape characters such as new line\n",
    "- Removing Hyperlinks\n",
    "- Removing Punctations\n",
    "- Removing Numbers\n",
    "- Removing stop words (a, an, the etc.)\n",
    "- Removing profane words if neccessary\n",
    "- Stemming or lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sorted-tactics",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:20:35.761826Z",
     "start_time": "2021-02-10T06:20:35.747858Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning lines\n",
    "\n",
    "# Collects hastags used in tweets\n",
    "hashtags = dict()\n",
    "def removing_usernames_and_collecting_hashtags(line):\n",
    "    data = line.split()\n",
    "    if len(data) < 1:\n",
    "        return line\n",
    "    \n",
    "    for item in data[1:] if data[0][0] == '@' else data: # Because after spliiting if userid present then it will always be at index 0 and will be key\n",
    "        if item[0] == '#':\n",
    "            if item in hashtags:\n",
    "                hashtags[item] += 1\n",
    "            else:\n",
    "                hashtags[item] = 1\n",
    "        elif item[0] == '@':\n",
    "            data.remove(item)\n",
    "    return \" \".join(data)\n",
    "\n",
    "def clean_line(line):\n",
    "    line = line.lower().encode('ascii', errors = 'ignore').decode() # Converting to lower case will remove emojis [pure text]\n",
    "    line = \" \".join(line.strip().split()) # Split will automatically remove escape characters\n",
    "    \n",
    "    # Removing links\n",
    "    line = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', '', line)\n",
    "    \n",
    "    # Collecting and removing usernames and hashtags\n",
    "    line = removing_usernames_and_collecting_hashtags(line)\n",
    "    \n",
    "    # Removing Punctuations except @ as only one username will be present\n",
    "    line = line.translate(str.maketrans('', '', '!\"$%&\\'()*+,-./:;#<=>?[\\\\]^_`{|}~'))\n",
    "    \n",
    "    # Removing Numbers\n",
    "    line = re.sub(r'\\d+', '', line)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Removing emojis\n",
    "    regrex_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\"]+\", re.UNICODE)\n",
    "    \n",
    "    line = regrex_pattern.sub(r'',line)\n",
    "    \"\"\"\n",
    "\n",
    "    # Removing stopwords\n",
    "    stopword = stopwords.words('english')\n",
    "    word_tokens = line.split()\n",
    "    line_words = [word for word in word_tokens if word not in stopword]\n",
    "    \n",
    "    # Removing Profanity Words\n",
    "    profanity = set()\n",
    "    with open('Profanity.txt', mode = 'r') as text_profanity:\n",
    "        reader = text_profanity.readlines()\n",
    "        for word in reader:\n",
    "            profanity.add(word.replace('\\n', ''))\n",
    "    line_words = [word for word in line_words if word not in profanity]\n",
    "    \n",
    "    # Stemming\n",
    "    #line_words = [PorterStemmer().stem(word) for word in line_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    line_words = [WordNetLemmatizer().lemmatize(word, pos = 'v') for word in line_words] # lemmatize parts of speech(pos)\n",
    "    \n",
    "    line = \" \".join(line_words)\n",
    "    return line    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "romance-short",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:20:35.776781Z",
     "start_time": "2021-02-10T06:20:35.763816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Reading data present in form of tweets in tweets.txt\\n\\n# Tweet_with_id for tweet analysis\\ntweet_with_id = dict()\\n\\n# Tweet with no _id for sentiment and other purposes\\ntweet_with_noid = []\\n\\nwith open('covid19.txt', mode = 'r', encoding = 'utf8') as text_file:\\n    reader = text_file.readlines() # Reading Lines\\n    for line in reader:\\n        if line == '\\n':\\n            continue\\n        line = clean_line(line) # Cleaning line by calling function\\n        if line == '': # If empty string\\n            continue\\n\\n        if line[0] == '@': # Getting the user_id of tweet\\n            words = line.split()\\n            if len(words) <= 1: # If user only posted link or non string, if len = 1, means only userid present\\n                continue\\n            tweet_with_id[words[0]] = line.replace(words[0], '', 1).strip() # Stripping whitespaces in tweets\\n        else:\\n            tweet_with_noid.append(line) # No need of stripping\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this code cell if file is downloaded into machine and dont want to use file on server\n",
    "\n",
    "# Reading data present in form of tweets in tweets.txt\n",
    "\n",
    "# Tweet_with_id for tweet analysis\n",
    "tweet_with_id = dict()\n",
    "\n",
    "# Tweet with no _id for sentiment and other purposes\n",
    "tweet_with_noid = []\n",
    "\n",
    "with open('covid19.txt', mode = 'r', encoding = 'utf8') as text_file:\n",
    "    reader = text_file.readlines() # Reading Lines\n",
    "    for line in reader:\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "        line = clean_line(line) # Cleaning line by calling function\n",
    "        if line == '': # If empty string\n",
    "            continue\n",
    "\n",
    "        if line[0] == '@': # Getting the user_id of tweet\n",
    "            words = line.split()\n",
    "            if len(words) <= 1: # If user only posted link or non string, if len = 1, means only userid present\n",
    "                continue\n",
    "            tweet_with_id[words[0]] = line.replace(words[0], '', 1).strip() # Stripping whitespaces in tweets\n",
    "        else:\n",
    "            tweet_with_noid.append(line) # No need of stripping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "atomic-dover",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:23:59.920081Z",
     "start_time": "2021-02-10T06:20:35.777779Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Use this code cell if file available on server and not on machine and properties and name of file match in the code\n",
    "# Reading data present in form of tweets in tweets.txt\n",
    "\n",
    "# Tweet_with_id for tweet analysis\n",
    "tweet_with_id = dict()\n",
    "\n",
    "# Tweet with no _id for sentiment and other purposes\n",
    "tweet_with_noid = []\n",
    "\n",
    "# Collecting zip file from url\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "# or: requests.get(url).content\n",
    "\n",
    "resp = urlopen(\"https://spotleai.sgp1.digitaloceanspaces.com/course/zip/covid19.txt.zip\")\n",
    "zipfile = ZipFile(BytesIO(resp.read()))\n",
    "\n",
    "for line in zipfile.open('covid19.txt').readlines():\n",
    "    line = line.decode('utf-8')\n",
    "    if line == '\\n':\n",
    "        continue\n",
    "    line = clean_line(line) # Cleaning line by calling function\n",
    "    if line == '': # If empty string\n",
    "        continue\n",
    "\n",
    "    if line[0] == '@': # Getting the user_id of tweet\n",
    "        words = line.split()\n",
    "        if len(words) <= 1: # If user only posted link or non string, if len = 1, means only userid present\n",
    "            continue\n",
    "        tweet_with_id[words[0]] = line.replace(words[0], '', 1).strip() # Stripping whitespaces in tweets\n",
    "    else:\n",
    "        tweet_with_noid.append(line) # No need of stripping\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "magnetic-remedy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:23:59.938752Z",
     "start_time": "2021-02-10T06:23:59.921105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8979"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of hashtags\n",
    "len(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reflected-condition",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:23:59.950678Z",
     "start_time": "2021-02-10T06:23:59.939560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3961"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tweets with userid\n",
    "len(tweet_with_id.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "straight-folder",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:23:59.965849Z",
     "start_time": "2021-02-10T06:23:59.951686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279724"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tweets with no userid\n",
    "len(tweet_with_noid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-parts",
   "metadata": {},
   "source": [
    "- ***Strings will not have escape characters***\n",
    "- ***Strings will not have whitespaces***\n",
    "- ***Strings will not have any links***\n",
    "- ***Strings will not have any emojis***\n",
    "- ***Strings will not have any punctuations except # and @ because we need them later.***\n",
    "- ***All text processing steps are done.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "trying-traffic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:24:00.200721Z",
     "start_time": "2021-02-10T06:23:59.967878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Storing cleaned data into machine\n",
    "with open('cleaned_data_with_id.csv', 'w', encoding = 'utf8') as filehandle:\n",
    "    writer = csv.writer(filehandle)\n",
    "    for key, value in tweet_with_id.items():\n",
    "        #key = str.encode(key).decode('utf-8')\n",
    "        #value = str.encode(value).decode('utf-8')\n",
    "        writer.writerow([key, value])\n",
    "        \n",
    "\n",
    "with open('cleaned_data_with_no_id.txt', 'w', encoding = 'utf8') as filehandle:\n",
    "    filehandle.writelines(\"%s\\n\" % place for place in tweet_with_noid)\n",
    "\n",
    "# Storing hashtags\n",
    "with open('hashtags.csv', 'w', encoding = 'utf8') as filehandle:\n",
    "    writer = csv.writer(filehandle)\n",
    "    for key, value in hashtags.items():\n",
    "        #key = str.encode(key).decode('utf-8')\n",
    "        #value = str.encode(value).decode('utf-8')\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "incorporated-newman",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:24:00.216426Z",
     "start_time": "2021-02-10T06:24:00.201721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading Cleaned Data\n",
    "df = pd.read_csv('cleaned_data_with_id.csv', header = None, names = ['userid', 'tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "broadband-debut",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:24:00.232265Z",
     "start_time": "2021-02-10T06:24:00.217425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>@michaelcoudrey</td>\n",
       "      <td>proof fauci deep state hat agenda use covid pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>@uberbratwurst</td>\n",
       "      <td>puzzle isnt wordcorona virus anything common c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>@godrejsecure</td>\n",
       "      <td>first fix product problems customer issue talk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323</th>\n",
       "      <td>@lakshmipriya</td>\n",
       "      <td>chamchas know covid stay cant stay lockdown fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>@actornikhil</td>\n",
       "      <td>film satya also get feature cnn ibns greatest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>@vitalvegas</td>\n",
       "      <td>dr birx say flu call corona wish video ive see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>@marcellelouise</td>\n",
       "      <td>thats fineso corona put whats kill themnot bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3806</th>\n",
       "      <td>@mytruthhurts</td>\n",
       "      <td>shorter scooby dont care many people get sick ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>@karlibarnett</td>\n",
       "      <td>include entire world look forward whole covid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>@scottpresler</td>\n",
       "      <td>hopefully still enough november unfortunately ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               userid                                              tweet\n",
       "1802  @michaelcoudrey  proof fauci deep state hat agenda use covid pr...\n",
       "2458   @uberbratwurst  puzzle isnt wordcorona virus anything common c...\n",
       "1955    @godrejsecure  first fix product problems customer issue talk...\n",
       "3323    @lakshmipriya  chamchas know covid stay cant stay lockdown fo...\n",
       "72       @actornikhil  film satya also get feature cnn ibns greatest ...\n",
       "1334      @vitalvegas  dr birx say flu call corona wish video ive see...\n",
       "2135  @marcellelouise  thats fineso corona put whats kill themnot bla...\n",
       "3806    @mytruthhurts  shorter scooby dont care many people get sick ...\n",
       "710     @karlibarnett  include entire world look forward whole covid ...\n",
       "607     @scottpresler  hopefully still enough november unfortunately ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Data\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-mortgage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
